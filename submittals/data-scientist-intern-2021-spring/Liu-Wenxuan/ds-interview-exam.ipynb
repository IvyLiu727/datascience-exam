{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n<h1> Bombora Data Science: <br> *Interview Exam* </h1>\n</div>\n\n<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">","metadata":{}},{"cell_type":"markdown","source":"---\n# Welcome\n\nWelcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`—please read that first, *before* attempting to answer questions here.\n\n<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n<p style=\"font-weight:bold\">ADVICE</p>\n<p>*Do not* read these questions, and panic, *before* reading the instructions in `README.md`.</p>\n</div>\n\n<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n<p style=\"font-weight:bold\">WARNING</p>\n\n<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and rember download notebook when you step away (you can always re-upload and start again)!</p>\n</div>\n\n\n## Have fun!\n\nRegardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!","metadata":{}},{"cell_type":"markdown","source":"# Exam Questions","metadata":{}},{"cell_type":"markdown","source":"## 1. Algo + Data Structures","metadata":{}},{"cell_type":"markdown","source":"### Q 1.1: Fibionacci\n![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.1.1\nGiven $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n\n$$ F_n = F_{n-1} + F_{n-2}$$\n\nwith initial conditions of:\n\n$$ F_1 = 1,  F_2 = 1$$","metadata":{}},{"cell_type":"code","source":"def fibonacci(n):\n    # Base cases\n    if n == 1:\n        return 1\n    if n == 2:\n        return 1\n    \n    # Recursion\n    f1 = fibonacci(n-1)\n    f2 = fibonacci(n-2)\n    \n    return f1 + f2","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Q 1.1.2\nWhat's the complexity of your implementation?","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAssume the cost of adding, subtracting, or multiplying two integers is O(1).\n\nThis is a recursive algorithm, where the recurrence is: \n        T(n) = T(n − 1) + T(n − 2) + d  if n > 2 for some constant d,\n        T(n) = c                        if n <= 2 (base case) for some constant c\n\nUsing recursion tree (depth = n/2), we can get a asymptotic lower bound on T(n):\n        T(n) = Omega(2^n) \n\nTherefore, the complexity of this implementation is exponential.\n\"\"\"","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\nAssume the cost of adding, subtracting, or multiplying two integers is O(1).\\n\\nThis is a recursive algorithm, where the recurrence is: \\n        T(n) = T(n − 1) + T(n − 2) + d  if n > 2 for some constant d,\\n        T(n) = c                        if n <= 2 (base case) for some constant c\\n\\nUsing recursion tree (depth = n/2), we can get a asymptotic lower bound on T(n):\\n        T(n) = Omega(2^n) \\n\\nTherefore, the complexity of this implementation is exponential.\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Q 1.1.3\nConsider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`.","metadata":{}},{"cell_type":"code","source":"def fibonacci2(n):\n    F = [0] * n\n    F[0] = 1\n    F[1] = 1\n    for i in range(2, n):\n        F[i] = F[i-1] + F[i-2]\n    return F[n-1]","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Q 1.1.4\nWhat's the complexity of your implementation?","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAssume the cost of adding, subtracting, or multiplying two integers is O(1).\n\nNote that this is a dynamic programming solution, where the for loop iterates n − 2 times, and each iteration takes O(1).\nThus, T(n) = (n-2) * O(1) = O(n).\nIn other words, this algorithm has linear complexity with respect to n, (but not necessary linear to the input size).\n\n\"\"\"","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'\\nAssume the cost of adding, subtracting, or multiplying two integers is O(1).\\n\\nNote that this is a dynamic programming solution, where the for loop iterates n − 2 times, and each iteration takes O(1).\\nThus, T(n) = (n-2) * O(1) = O(n).\\nIn other words, this algorithm has linear complexity with respect to n, (but not necessary linear to the input size).\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Q 1.1.5\nWhat are some examples of optimizations that could improve computational performance?\n","metadata":{}},{"cell_type":"code","source":"\"\"\"\n1. Use data structures that are suitable/appropriate for the specific operations and hardware. For example, if we do not care about the order of the data but we need to\n   perform many look-up operations, then dictionaries should be preferred over lists.\n   \n2. Optimize the data itself so that it requires less space and computational power. For example, if we have only two strings: \"blue\" and \"yellow\", we can simply store them\n   as two ints: 0 and 1.\n\n3. Select efficient algorithm. For example, to calculate the fibonacci number, the DP algorithm performs much faster than the recursion algorithm (when n is large) by \n   avoiding repeating calculations of the same value.\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q 1.2: Linked List\n![ll img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Singly-linked-list.svg/500px-Singly-linked-list.svg.png)","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.1\nConsider a [singly linked list](https://en.wikipedia.org/wiki/Linked_list), $L$. Write a function `is_palindrome(L)` that detects if $L$ is a [palindrome](https://en.wikipedia.org/wiki/Palindrome), by returning a bool, `True` or `False`.\n","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.2\nWhat is the complexity of your implementation?","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.3\nConsider an alternative implementation to detect if L is a palindrome and write a new function, `is_palindrome2(L)`.","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.4\nWhat's the complexity of this implementation?\n","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.5 \nWhat are some examples of optimizations that could improve computational performance?\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Prob + Stats","metadata":{}},{"cell_type":"markdown","source":"### Q 2.1: Finding $\\pi$ in a random uniform?\n<img src=https://www.epicurus.com/food/recipes/wp-content/uploads/2015/03/Pi-Day.jpg width=\"480\">\n\nGiven a uniform random generator $[0,1)$ (e.g., use your language's standard libary to generate random value), write a a function `compute_pi` to compute [$\\pi$](https://en.wikipedia.org/wiki/Pi).","metadata":{}},{"cell_type":"markdown","source":"### Q 2.2: Making a 6-side die roll a 7?\n\nUsing a single 6-side die, how can you generate a random number between 1 - 7?","metadata":{}},{"cell_type":"markdown","source":"### Q 2.3: Is normality uniform?\n\n<img src=https://rednaxela1618.files.wordpress.com/2014/06/uniformnormal.png width=\"480\">\n\n\nGiven draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?","metadata":{}},{"cell_type":"markdown","source":"### Q 2.4: Should you pay or should you go?\n\n![coin flip](https://lh5.ggpht.com/iwD6MnHeHVAXNBgrO7r4N9MQxxYi6wT9vb0Mqu905zTnNlBciONAA98BqafyjzC06Q=w300)\n\nLet’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you $2^{(n-1)}$ US dollars. How much would you pay me to play this game? Explain.","metadata":{}},{"cell_type":"markdown","source":"### Q 2.5: Uber vs. Lyft\n\n![uber vs lyft](http://usiaffinity.typepad.com/.a/6a01347fc1cb08970c01bb0876bcbe970d-pi)\n\nYou request 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?","metadata":{}},{"cell_type":"markdown","source":"### Q 2.6: Pick your prize\n<img src=https://miro.medium.com/max/1100/1*m5b3O9sE68UCXjLw5oxy2g.png width=\"480\">\n\nA prize is placed at random behind one of three doors and you are asked to pick a door. To be concrete, say you always pick door 1. Now the game host chooses one of door 2 or 3, opens it and shows you that it is empty. They then give you the option to keep your picked door or switch to the unopened door. Should you stay or switch if you want to maximize your probability of winning the prize?","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAnswer:\n    I should switch.\n\nExplanation:\n    Let P(i) = the probability that the prize is placed behind door i (i = 1, 2, or 3)\n    Initially, we have P(i) = 1/3\n\n    After picking door 1, let's suppose that the host opens door 2. (The analysis is the same if the host opens door 3, just replace \"host opens door 2\" with \"host opens door 3\")\n\n    Case 1: The prize is behind door 1, then the host can open either door 2 or 3 with equal probability. \n    Case 2: The prize is behind door 2, then the host opens door 3, that is, cannot open door 2.\n    Case 3: The prize is behind door 3, then the host definitely opens door 2.\n    \n    Translate into probabilities:\n    P(host opens door 2 | prize behind door 1) = 1/2\n    P(host opens door 2 | prize behind door 2) = 0\n    P(host opens door 2 | prize behind door 3) = 1\n    \n    According to Bayes' Formula:\n    P(host opens door 2) = P(host opens door 2 | prize behind door 1) * P(1) + P(host opens door 2 | prize behind door 2) * P(2) + P(host opens door 2 | prize behind door 3) * P(3)\n                        = 1/2 * 1/3 + 0 + 1 * 1/3 \n                        = 1/2\n    \n    P(switching and winning) = P(prize behind door 3 | host opens door 2)  \n                             = P(door 2 is opened and prize behind door 3) /  P(host opens door 2) \n                             = P(host opens door 2 | prize behind door 3) * P(3) /  P(host opens door 2) \n                             = 1/3 / 1/2\n                             = 2/3\n    P(staying and winning) = P(prize behind door 1 | host opens door 2)  \n                             = P(door 2 is opened and prize behind door 1) /  P(host opens door 2) \n                             = P(host opens door 2 | prize behind door 1) * P(1) /  P(host opens door 2)\n                             = 1/6 / 1/2\n                             = 1/3\n\n    No matter which door the host opens, P(switching and winning) > P(staying and winning). Thus, I should switch.\n    \n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 Conceptual ML","metadata":{}},{"cell_type":"markdown","source":"### Q 3.1 Why study gradient boosting or neural networks?\n\nConsider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n\\begin{equation}\nEPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n\\end{equation}\nwhere EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n\\begin{equation}\nf(x) = \\mathbb{E}\\left[Y|X=x\\right]\n\\end{equation}\n\nThe goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$ while a neural network provides a nonlinear approximation of the regression function. \n\nThe simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighbourhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n\nNow given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAlthough theoretically, KNN algorithm converges to ideal solution as the training sample and number of neighbors increases to infinity, it has limitations.\n\n1. KNN does not work efficiently with large sample sizes as compared to smaller sample sizes. In other words, KNN does not scale well as the number of distance calculation\n   grows. However, having a large training sample size is one of the requirements for optimal performance. Therefore, we need to explore algorithms that can compensate this\n   issue.\n   \n2. As the dimension of the data grows, KNN performs worse as it is harder to obtain the distance between data of different dimensions. In real world scenarios, we often face\n   situations where it requires classification of data of high dimension. For example, we would like to classify music tracks based on artist, produced date, length, genre,\n   mood, country, background, etc. In such cases, neural networks can produce more accurate classifications.\n\n3. KNN does not perform well if the data set is incomplete as it gives inaccurate neighbors when there is a lot of missing data. \n\n4. KNN is a supervised leanring algorithm. Therefore, for cases where the data is not labeled, KNN cannot be used.\n\n5. The choice of k can siginificantly affect the classification result.\n\nThese limitations provide motivations for further research on classification algorithms that can produce overcome these constraints and give higher accuracy results. \nSpecifically, gradient boosting uses the idea of combining simple classification models (weak learners) together to gather more information. Neural networks, on the other \nhand, aims to mimic the human brain to make complex predictions.\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q 3.2 Model Selection and Assesment\n\nConsider a multiclass classification problem with a large number of features $p >> N$, for e.g $p=10000, N=100$ The task is threefold\n1. Find a \"good\" subset of features that show strong _univariate_ correlation with class labels\n2. Using the \"good\" subset, build a multi class classifier\n3. Estimate the generalization error of the final model\n\nGiven this dataset, outline your approach and please be sure to cover the following\n- Data splitting\n- Model Selection: either estimating the performance of different classifiers or the same classifier with different hyperparameters\n- Model Assessment: having chosen a classifier, estimating the generalization error\n\nAssume all features are numerical, the dataset contains no NULLS, outliers, etc. and doesn't require any preprocessing.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}